\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{amssymb,amsmath,clrscode,graphicx,indentfirst}

\author{Олег Смирнов}
\title{Курс kiev-clrs -- Лекция 15. Динамическое программирование}
\date{4 июля 2009 г.}

\begin{document}
\maketitle
\tableofcontents

\newpage
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\section{Цель лекции}
\begin{itemize}
\item Динамическое программирование и его приложения
\item Мемоизация
\end{itemize}

\section{Введение}
Слово ``программирование'' в названии метода не относится к программированию как к написанию программ, так же как и в ``линейном программировании''. Исторически так называли табличные методы, как в случае перфокарт.

Динамическое программирование является методом дизайна алгоритмов, так же как ``разделяй и властвуй'', ``жадные алгоритмы'', ``рандомизация'' и т.д.

\section{Наибольшая общая подпоследовательность}
Хорошим примером применения метода является задача поиска наибольшей общей подпоследовательности (Longest Common Subsequence, LCS).

Для двух заданных строк (множест элементов) $x[1 \twodots m]$ и $y[1 \twodots n]$ найти наибольшие общие последовательности элементов (их может быть несколько).

Задача возникает, например, в вычислительное биологии, где требуется сравнивать цепочки ДНК.

В этом примере $LCS(x, y) = BCBA, BDAB, BCAB, \twodots$.
\begin{center}
\begin{tabular}{|r|c|c|c|c|c|c|c|}
  \hline
     $x$ & A & B & C & B & D & A & B \\
  \hline
     $y$ & B & D & C & A & B & A &   \\
  \hline
\end{tabular}
\end{center}

В случае наивного алгоритма поиска $LCS$ методом brute force: для каждой подпоследовательности из $x$ проверить, содержится ли она в $y$. Тогда:
\begin{itemize}
\item время проверки каждой подпоследовательности в $y$: $O(n)$
\item общее количество подпоследовательностей в $x$: $2^m$ (рассматривая вектор битов длинной $m$, каждый бит которого означает, включать ли соответствующий элемент)
\item время работы алгоритма в худшем случае: $O(n2^m)$ -- экспоненциально
\end{itemize}

Более оптимальный алгоритм поиска $LCS$ состоит из двух шагов:
\begin{enumerate}
\item Вычисление \emph{длины} наибольшей общей подпоследовательности
\item Поиск подпоследовательности заданной длины
\end{enumerate}
Очевидно, что наибольшая длина, полученная на первом шаге, является искомой. Алгоритм становится проще за счёт того, что необходимо работать только с числами -- длинами $LCS$. Обозначим длину последовательности $s$ через $|s|$.

Важным шагом в дизайне алгоритмов динамического программирования является рассмотрения подзадач. Необходимо показать, что оптимальное решение исходной задачи содержит оптимальные решения подзадач меньшего размера.

Рассмотрим \emph{префиксы} $x$ и $y$. 

Пусть $c[i, j] = |LCS(x[1 \twodots i], y[1 \twodots j]|$ -- длина общей подпоследовательности для префиксов $x$ и $y$ длины $i$ и $j$ соответственно. Вычислив для всех $i$ и $j$, получим $c[m, n] = |LCS(x, y)|$ -- искомая наибольшая подпоследовательность.

Значение $c[i, j]$ можно вычислить рекурсивно. Докажем теорему:
\begin{equation*}
  c[i, j] = \begin{cases}
    c[i-1, j-1] + 1, \text{ если } x[i] = y[j] \\
    max\{c[i-1, j], c[i, j-1]\}, \text{ в противном случае}
    \end{cases}
\end{equation*}
Т.е. длина общей последовательности двух префиксов на единицу больше предыдущих префиксов, если их конечные элементы совпадают, или равна максимуму из двух длин для двух предыдущих префиксов в противном случае.
\begin{figure}[ht]
  \centering
  \includegraphics[width=3.5in]{lecture15/lcs.eps}
  \caption{Задача LCS}
\end{figure}

Рассмотрим случай $x[i] = y[j]$.

Пусть $z[1 \twodots k] = LCS(x[1 \twodots i], y[1 \twodots j])$ -- $LCS$ префиксов, тогда $c[i, j] = k$ -- длина $z$. Тогда последний символ подпоследовательности должен быть равен $z[k] = x[i] (= y[j])$ иначе $z$ можно было бы удлинить, добавив $x[i]$.

Следствие: $z[1 \twodots k-1]$ является наибольшей подпоследовательностью префиксов: 
\begin{equation*}
  z[1 \twodots k-1] = LCS(x[1 \twodots i-1], y[1 \twodots j-1])  
\end{equation*}

Пусть $w$ -- более длинная подпоследовательность $x[1 \twodots i-1]$ и $y[1 \twodots j-1]$, т.е. $|w| > k-1$. Тогда (аргумент ``cut and paste'') конкатенация $w \parallel z[k]$ также является общей подпоследовательностью $x[1 \twodots i]$, $y[1 \twodots j]$ длины $|w \parallel z[k]| > k$. Это противоречит доказывает следствие.

Таким образом $c[i-1, j-1] = k-1$, откуда следует, что $c[i, j] = c[i-1, j-1] +1$. Остальные случаи симметричны.

Это свойство называется ``оптимальной подструктурой'' или первым признаком динамического программирования.

\emph{Первый признак динамического программирования}: оптимальное решение исходной задачи содержит оптимальные решения подзадач меньшего размера.

В случаи задачи $LCS$ это означает, что если $z = LCS(x, y)$, то любой префикс $z$ является $LCS$ префикса $x$ и префикса $y$.

Для проверки первого признака обычно используют аргумент ``cut and paste''.

Испльзуя теорему, можно записать рекурсивный алгоритм:
\begin{codebox}
\Procname{$\proc{LCS}(x, y, i, j)$}
\li \If $x[i] = y[i]$
\li   \Then $c[i,j] \gets LCS(x, y, i-1, i-1)+1$
\li   \Else $c[i,j] \gets max\{LCS(x, y, i-1, j), LCS(x, y, i, j-1)\}$
    \End
\li \Return $c[i,j]$
\end{codebox}
В худшем случае, если $x[i] \neq y[j]$ алгоритм выполняет рекурсивное вычисление двух подзадач, по очереди декрементируя один из параметров.
\begin{figure}[h!]
  \centering
  \includegraphics[width=3.5in]{lecture15/tree.eps}
  \caption{Дерево рекурсии}
\end{figure}
Дерево рекурсии алгоритма имеет высоту $m+n$, т.е. время выполнения по прежнему будет экспоненциально.

Можно заметить, что некоторые поддеревья в нём повторяются несколько раз. Вычисляя соответствующие подзадачи однократно, время работы можно сократить.

Это свойство называется ``перекрывающимися подзадачами'' или вторым признаком динамического программирования.

\emph{Второй признак динамического программирования}: рекурсивное решение содержит небольшое количество различных подзадач, которые повторяются многократно.

Количество различных подзадач в $LCS$ равно $nm$, что гораздо меньше $n2^m$.

\section{Мемоизация}
Для решения можно применить подход, называемый мемоизацией (memoization): после вычисления решения подзадачи оно записывается в таблицу и при повторном вызове уже не вычисляется, а берется из таблицы.

\begin{codebox}
\Procname{$\proc{LCS}(x, y, i, j)$}
\li \If $c[i,j] = NIL$
\li \Then \If $x[i] = y[i]$
\li     \Then $c[i,j] \gets LCS(x, y, i-1, i-1)+1$
\li     \Else $c[i,j] \gets max\{LCS(x, y, i-1, j), LCS(x, y, i, j-1)\}$
      \End
    \End
\li \Return $c[i,j]$
\end{codebox}
Амортизированное время работы этого алгоритма $T(m, n) = \Theta(mn)$. Объем используемой памяти $Space(m, n) = \Theta(mn)$.

Мемоизация хорошо подходит для функций, которые возвращают одинаковые значения для одинаковых входных параметров. Этот приём не работает для функций с побочными эффектами.

\section{Динамическое программирование}
Алгоритм с мемоизацией использует значительные объем памяти и выполняет вычисления в обратном порядке (сверху-вниз). Идея динамического программирования, заключается в том, что таблицу значений $c[i,j]$ можно вычислять последовательно, снизу-вверх:
\begin{figure}[h!]
  \centering
  \includegraphics[width=3in]{lecture15/table1.eps}
  \caption{Таблица LCS}
\end{figure}

Порядок заполнения таблицы:
\begin{itemize}
\item первая строка таблицы заполняется нулями
\item каждый элемент последовательно заполняется по формуле для $c[i, j]$: если символы для позиции $i, j$ совпадают, то в неё записывается значение $c[i-1, j-1]+1$, иначе вычисляется максимум от соседей слева и сверху
\end{itemize}
Элемент в правом нижнем углу показывает длину наибольшей общей подпоследовательности. Для построения искомой последовательности нужно пройти по таблице в обратную сторону.

Порядок прохода по таблице:
\begin{itemize}
\item если элемент получен как максимум от двух префиксов -- перейти в позицию с наибольшим префиксом
\item если элемент получен прибавлением единицы к соседу по диагонали -- перейти в эту позицию и добавить соответствующий символ к последовательности
\item выбирая различные эквивалентные пути можно получить все возможные последовательности
\end{itemize}
\begin{figure}[h!]
  \centering
  \includegraphics[width=3in]{lecture15/table2.eps}
  \caption{Таблица LCS}
\end{figure}

Алгоритм использует $\Theta(mn)$ памяти. Объем памяти можно сократить до $O(min\{m, n\})$, если хранить вместо всей таблицы только одну (предыдущую) строчку, которая необходима для вычисления элемента.

\section{Задачи динамического програмимрования}

Рассмотренный алгоритм LCS используется, в частности, в программе diff. Существует еще ряд известных задач, решаемых методом динамического программимрования.

\subsection{Редакторское расстояние}

Расстоянием Левенштейна (также редакторским расстояним, editor's distance) называется мера разницы двух последовательностей символов (строк) относительно минимального количества операций вставки, удаления и замены, необходимых для перевода одной строки в другую.

Задача поиска дистанции Левенштейна решается с помощью динамического программирования. Дистанция используется в алгоритма fuzzy matching (например, проверка правописания) и в текстовых редакторах.

\subsection{Перемножение матриц}

Число итераций, необходимое для умножения двух матриц размера $p \times q$ и $q \times r$ равно $p \cdot q \cdot r$. Умножение матриц ассоциативно, т.е. три матрицы $A_1, A_2, A_3$ размерности $10 \times 100$, $100 \times 5$ и $5 \times 50$ соответственно, можно перемножить двумя возможными способами: $((A_1 \cdot A_2) \cdot A_3)$ и $(A_1 \cdot (A_2 \cdot A_3))$. Однако в первом случае будет выполнено$10 \cdot 100 \cdot 5 + 10 \cdot 5 \cdot 50 = 7500$ итераций. А во втором -- $100 \cdot 5 \cdot 50 + 10 \cdot 100 \cdot 50 = 75000$ итераций.

Можно показать, что число возможных вариантов перемножения нескольких матриц равно числу вариантов расстановки скобок между ними, которое равно числу Каталана $C_n \sim \frac{4^n}{n^{3/2}}$ -- оно растёт экспоненциально.

С помощью динамического программирования, можно найти оптимальный порядок перемножения матриц за $O(n^3)$ итераций.

\subsection{Поиск оптимального пути в графе}

Задача поиска поиска оптимальных путей из одной вершины нагруженного графа в остальные решается алгоритмом Дейкстры (для графа без дуг отрицательного веса) и алгоритмом Беллмана-Форда (с ними). 

При решении каждой вершине из графа $V$ сопоставляется метка -- минимальное известное расстояние от этой вершины до начальной $a$. Алгоритм работает пошагово -— на каждом шаге он ``посещает'' одну вершину и пытается уменьшать метки. Работа завершается, когда все вершины посещены. В начале работы метки вершин принимаются равными бесконечности. На каждом шаге вершины просматриваются в порядке BFS (поиск в ширину).

Практическим приложением этого алгоритма является протокол сетевой маршрутизации OSPF, который вычисляет оптимальный путь между узлами сети по заданной таблице маршрутизации.

\section{Заключительные замечания}

Мемоизация очень удобна в строго функциональных языках программирования, благодаря отстутствию побочных эффектов и наличию ленивых вычислений.

\end{document}
